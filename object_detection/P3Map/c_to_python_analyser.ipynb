{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json,natsort\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_data_file = './combined_uarch.csv'\n",
    "high_level_data_file = './all_config_stacked.csv'\n",
    "mapping_file = './mapping_funcs.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data as csv\n",
    "hw_df = pd.read_csv(hw_data_file, index_col=0)\n",
    "high_level_df = pd.read_csv(high_level_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Function</th>\n",
       "      <th>CPU Time (s)</th>\n",
       "      <th>Retiring</th>\n",
       "      <th>Front-End Bound</th>\n",
       "      <th>Bad Speculation</th>\n",
       "      <th>L1 Bound</th>\n",
       "      <th>L2 Bound</th>\n",
       "      <th>L3 Bound</th>\n",
       "      <th>Memory Bandwidth</th>\n",
       "      <th>Local Memory</th>\n",
       "      <th>Remote Memory</th>\n",
       "      <th>Remote Cache</th>\n",
       "      <th>Store Bound</th>\n",
       "      <th>Core Bound</th>\n",
       "      <th>CPU Time %</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ImagingResampleVertical_8bpc</td>\n",
       "      <td>614.306</td>\n",
       "      <td>1.780401e+14</td>\n",
       "      <td>4.337152e+11</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.951718e+12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.337152e+11</td>\n",
       "      <td>1.951718e+12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.168576e+11</td>\n",
       "      <td>3.773322e+13</td>\n",
       "      <td>3.390447</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ImagingResampleHorizontal_8bpc</td>\n",
       "      <td>385.980</td>\n",
       "      <td>1.040783e+14</td>\n",
       "      <td>4.920960e+11</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.611680e+11</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.460480e+11</td>\n",
       "      <td>1.107216e+12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.759243e+13</td>\n",
       "      <td>2.130282</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>at::native::copy_kernel(float)</td>\n",
       "      <td>358.990</td>\n",
       "      <td>6.974640e+12</td>\n",
       "      <td>1.679080e+12</td>\n",
       "      <td>6.458000e+11</td>\n",
       "      <td>1.097860e+13</td>\n",
       "      <td>1.291600e+13</td>\n",
       "      <td>1.265768e+13</td>\n",
       "      <td>1.021656e+14</td>\n",
       "      <td>1.291600e+14</td>\n",
       "      <td>1.679080e+12</td>\n",
       "      <td>5.166400e+11</td>\n",
       "      <td>1.154690e+14</td>\n",
       "      <td>7.103800e+12</td>\n",
       "      <td>1.981320</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>decode_mcu</td>\n",
       "      <td>357.255</td>\n",
       "      <td>3.390039e+13</td>\n",
       "      <td>6.625203e+12</td>\n",
       "      <td>3.510497e+13</td>\n",
       "      <td>1.471311e+13</td>\n",
       "      <td>1.720832e+11</td>\n",
       "      <td>8.604160e+10</td>\n",
       "      <td>6.797286e+12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.604160e+10</td>\n",
       "      <td>6.194995e+12</td>\n",
       "      <td>1.971744</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THFloatStorage_copy</td>\n",
       "      <td>319.535</td>\n",
       "      <td>3.630283e+13</td>\n",
       "      <td>4.116816e+12</td>\n",
       "      <td>3.742560e+12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.491072e+12</td>\n",
       "      <td>1.871280e+12</td>\n",
       "      <td>4.580893e+13</td>\n",
       "      <td>6.444688e+13</td>\n",
       "      <td>7.485120e+13</td>\n",
       "      <td>4.326399e+13</td>\n",
       "      <td>2.245536e+12</td>\n",
       "      <td>1.669182e+13</td>\n",
       "      <td>1.763562</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Function  CPU Time (s)      Retiring  \\\n",
       "0    ImagingResampleVertical_8bpc       614.306  1.780401e+14   \n",
       "1  ImagingResampleHorizontal_8bpc       385.980  1.040783e+14   \n",
       "2  at::native::copy_kernel(float)       358.990  6.974640e+12   \n",
       "3                      decode_mcu       357.255  3.390039e+13   \n",
       "4             THFloatStorage_copy       319.535  3.630283e+13   \n",
       "\n",
       "   Front-End Bound  Bad Speculation      L1 Bound      L2 Bound      L3 Bound  \\\n",
       "0     4.337152e+11     0.000000e+00  1.951718e+12  0.000000e+00  4.337152e+11   \n",
       "1     4.920960e+11     0.000000e+00  8.611680e+11  0.000000e+00  2.460480e+11   \n",
       "2     1.679080e+12     6.458000e+11  1.097860e+13  1.291600e+13  1.265768e+13   \n",
       "3     6.625203e+12     3.510497e+13  1.471311e+13  1.720832e+11  8.604160e+10   \n",
       "4     4.116816e+12     3.742560e+12  0.000000e+00  4.491072e+12  1.871280e+12   \n",
       "\n",
       "   Memory Bandwidth  Local Memory  Remote Memory  Remote Cache   Store Bound  \\\n",
       "0      1.951718e+12  0.000000e+00   0.000000e+00  0.000000e+00  2.168576e+11   \n",
       "1      1.107216e+12  0.000000e+00   0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "2      1.021656e+14  1.291600e+14   1.679080e+12  5.166400e+11  1.154690e+14   \n",
       "3      6.797286e+12  0.000000e+00   0.000000e+00  0.000000e+00  8.604160e+10   \n",
       "4      4.580893e+13  6.444688e+13   7.485120e+13  4.326399e+13  2.245536e+12   \n",
       "\n",
       "     Core Bound  CPU Time %  config  \n",
       "0  3.773322e+13    3.390447       4  \n",
       "1  1.759243e+13    2.130282       4  \n",
       "2  7.103800e+12    1.981320       4  \n",
       "3  6.194995e+12    1.971744       4  \n",
       "4  1.669182e+13    1.763562       4  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config</th>\n",
       "      <th>Collation</th>\n",
       "      <th>Load</th>\n",
       "      <th>Normalize</th>\n",
       "      <th>RandomHorizontalFlip</th>\n",
       "      <th>Resize</th>\n",
       "      <th>ToTensor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>433.549978</td>\n",
       "      <td>1125.375266</td>\n",
       "      <td>915.226421</td>\n",
       "      <td>61.015711</td>\n",
       "      <td>1106.893002</td>\n",
       "      <td>791.916118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   config   Collation         Load   Normalize  RandomHorizontalFlip  \\\n",
       "0       4  433.549978  1125.375266  915.226421             61.015711   \n",
       "\n",
       "        Resize    ToTensor  \n",
       "0  1106.893002  791.916118  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_level_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make config column the index\n",
    "high_level_df = high_level_df.set_index('config')\n",
    "\n",
    "# map config column with format 'b1024_gpu4_dataloader8' to '8' casted to int\n",
    "# hw_df['config'] = hw_df['config'].map(lambda x: int(x.split('_')[-1].replace('dataloader', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers are %s (not multiplied by 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Collation</th>\n",
       "      <th>Load</th>\n",
       "      <th>Normalize</th>\n",
       "      <th>RandomHorizontalFlip</th>\n",
       "      <th>Resize</th>\n",
       "      <th>ToTensor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>config</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>433.549978</td>\n",
       "      <td>1125.375266</td>\n",
       "      <td>915.226421</td>\n",
       "      <td>61.015711</td>\n",
       "      <td>1106.893002</td>\n",
       "      <td>791.916118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Collation         Load   Normalize  RandomHorizontalFlip  \\\n",
       "config                                                              \n",
       "4       433.549978  1125.375266  915.226421             61.015711   \n",
       "\n",
       "             Resize    ToTensor  \n",
       "config                           \n",
       "4       1106.893002  791.916118  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Numbers are %s (not multiplied by 100)')\n",
    "high_level_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Function</th>\n",
       "      <th>CPU Time (s)</th>\n",
       "      <th>Retiring</th>\n",
       "      <th>Front-End Bound</th>\n",
       "      <th>Bad Speculation</th>\n",
       "      <th>L1 Bound</th>\n",
       "      <th>L2 Bound</th>\n",
       "      <th>L3 Bound</th>\n",
       "      <th>Memory Bandwidth</th>\n",
       "      <th>Local Memory</th>\n",
       "      <th>Remote Memory</th>\n",
       "      <th>Remote Cache</th>\n",
       "      <th>Store Bound</th>\n",
       "      <th>Core Bound</th>\n",
       "      <th>CPU Time %</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ImagingResampleVertical_8bpc</td>\n",
       "      <td>614.306</td>\n",
       "      <td>1.780401e+14</td>\n",
       "      <td>4.337152e+11</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.951718e+12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.337152e+11</td>\n",
       "      <td>1.951718e+12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.168576e+11</td>\n",
       "      <td>3.773322e+13</td>\n",
       "      <td>3.390447</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ImagingResampleHorizontal_8bpc</td>\n",
       "      <td>385.980</td>\n",
       "      <td>1.040783e+14</td>\n",
       "      <td>4.920960e+11</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.611680e+11</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.460480e+11</td>\n",
       "      <td>1.107216e+12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.759243e+13</td>\n",
       "      <td>2.130282</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>at::native::copy_kernel(float)</td>\n",
       "      <td>358.990</td>\n",
       "      <td>6.974640e+12</td>\n",
       "      <td>1.679080e+12</td>\n",
       "      <td>6.458000e+11</td>\n",
       "      <td>1.097860e+13</td>\n",
       "      <td>1.291600e+13</td>\n",
       "      <td>1.265768e+13</td>\n",
       "      <td>1.021656e+14</td>\n",
       "      <td>1.291600e+14</td>\n",
       "      <td>1.679080e+12</td>\n",
       "      <td>5.166400e+11</td>\n",
       "      <td>1.154690e+14</td>\n",
       "      <td>7.103800e+12</td>\n",
       "      <td>1.981320</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>decode_mcu</td>\n",
       "      <td>357.255</td>\n",
       "      <td>3.390039e+13</td>\n",
       "      <td>6.625203e+12</td>\n",
       "      <td>3.510497e+13</td>\n",
       "      <td>1.471311e+13</td>\n",
       "      <td>1.720832e+11</td>\n",
       "      <td>8.604160e+10</td>\n",
       "      <td>6.797286e+12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.604160e+10</td>\n",
       "      <td>6.194995e+12</td>\n",
       "      <td>1.971744</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THFloatStorage_copy</td>\n",
       "      <td>319.535</td>\n",
       "      <td>3.630283e+13</td>\n",
       "      <td>4.116816e+12</td>\n",
       "      <td>3.742560e+12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.491072e+12</td>\n",
       "      <td>1.871280e+12</td>\n",
       "      <td>4.580893e+13</td>\n",
       "      <td>6.444688e+13</td>\n",
       "      <td>7.485120e+13</td>\n",
       "      <td>4.326399e+13</td>\n",
       "      <td>2.245536e+12</td>\n",
       "      <td>1.669182e+13</td>\n",
       "      <td>1.763562</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Function  CPU Time (s)      Retiring  \\\n",
       "0    ImagingResampleVertical_8bpc       614.306  1.780401e+14   \n",
       "1  ImagingResampleHorizontal_8bpc       385.980  1.040783e+14   \n",
       "2  at::native::copy_kernel(float)       358.990  6.974640e+12   \n",
       "3                      decode_mcu       357.255  3.390039e+13   \n",
       "4             THFloatStorage_copy       319.535  3.630283e+13   \n",
       "\n",
       "   Front-End Bound  Bad Speculation      L1 Bound      L2 Bound      L3 Bound  \\\n",
       "0     4.337152e+11     0.000000e+00  1.951718e+12  0.000000e+00  4.337152e+11   \n",
       "1     4.920960e+11     0.000000e+00  8.611680e+11  0.000000e+00  2.460480e+11   \n",
       "2     1.679080e+12     6.458000e+11  1.097860e+13  1.291600e+13  1.265768e+13   \n",
       "3     6.625203e+12     3.510497e+13  1.471311e+13  1.720832e+11  8.604160e+10   \n",
       "4     4.116816e+12     3.742560e+12  0.000000e+00  4.491072e+12  1.871280e+12   \n",
       "\n",
       "   Memory Bandwidth  Local Memory  Remote Memory  Remote Cache   Store Bound  \\\n",
       "0      1.951718e+12  0.000000e+00   0.000000e+00  0.000000e+00  2.168576e+11   \n",
       "1      1.107216e+12  0.000000e+00   0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "2      1.021656e+14  1.291600e+14   1.679080e+12  5.166400e+11  1.154690e+14   \n",
       "3      6.797286e+12  0.000000e+00   0.000000e+00  0.000000e+00  8.604160e+10   \n",
       "4      4.580893e+13  6.444688e+13   7.485120e+13  4.326399e+13  2.245536e+12   \n",
       "\n",
       "     Core Bound  CPU Time %  config  \n",
       "0  3.773322e+13    3.390447       4  \n",
       "1  1.759243e+13    2.130282       4  \n",
       "2  7.103800e+12    1.981320       4  \n",
       "3  6.194995e+12    1.971744       4  \n",
       "4  1.669182e+13    1.763562       4  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the mapping functions\n",
    "mapping_funcs = json.load(open(mapping_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'op_to_func': {'BatchCollator': ['munmap|libc.so.6',\n",
       "   'gomp_team_end|libgomp.so.1',\n",
       "   'c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(float)#1}, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(float)#1}&&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2}>(, signed char, at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(float)#1}, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(float)#1}&&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2} const&)::{lambda(char**long const*, long, long)#1}>|libtorch_cpu.so',\n",
       "   '[OpenMP worker]|libgomp.so.1',\n",
       "   'gomp_simple_barrier_wait|libgomp.so.1',\n",
       "   'c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#1}, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#1}&&, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#2}&&, long)::{lambda(char**long const*, long)#2}>(, signed char, at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#1}, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#1}&&, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#2}&&, long)::{lambda(char**long const*, long)#2} const&)::{lambda(char**long const*, long, long)#1}>|libtorch_cpu.so',\n",
       "   '_ZN2at8internal15invoke_parallelIZNS_18TensorIteratorBase8for_eachEN3c1012function_refIFvPPcPKlllEEElEUlllE_EEvlllRKT_._omp_fn.0|libtorch_cpu.so',\n",
       "   'gomp_finish_task|libgomp.so.1',\n",
       "   'gomp_team_barrier_wait_end|libgomp.so.1'],\n",
       "  'RandomHorizontalFlip': ['ImagingFlipLeftRight|_imaging.cpython-38-x86_64-linux-gnu.so',\n",
       "   '_int_free|libc.so.6'],\n",
       "  'Load': ['__GI___libc_malloc|libc.so.6',\n",
       "   'method_vectorcall|python3.8',\n",
       "   'PyObject_IsTrue|python3.8',\n",
       "   'ImagingJpegDecode|_imaging.cpython-38-x86_64-linux-gnu.so',\n",
       "   'jpeg_read_scanlines|libjpeg.so.9',\n",
       "   'jpeg_fill_bit_buffer|libjpeg.so.9',\n",
       "   '__libc_calloc|libc.so.6',\n",
       "   'jpeg_idct_16x16|libjpeg.so.9',\n",
       "   'ycc_rgb_convert|libjpeg.so.9',\n",
       "   'PyObject_IsInstance|python3.8',\n",
       "   'ImagingUnpackRGB|_imaging.cpython-38-x86_64-linux-gnu.so',\n",
       "   '__memset_avx2_unaligned_erms|libc.so.6',\n",
       "   'bytes_concat|python3.8',\n",
       "   'process_data_simple_main|libjpeg.so.9',\n",
       "   'load|ImageFile.py',\n",
       "   'method_vectorcall_FASTCALL|python3.8',\n",
       "   'decompress_onepass|libjpeg.so.9',\n",
       "   'decode_mcu|libjpeg.so.9',\n",
       "   '__memmove_avx_unaligned_erms|libc.so.6',\n",
       "   '_copy|_imaging.cpython-38-x86_64-linux-gnu.so',\n",
       "   'jpeg_idct_islow|libjpeg.so.9',\n",
       "   '_decode|_imaging.cpython-38-x86_64-linux-gnu.so'],\n",
       "  'Resize': ['ImagingResampleHorizontal_8bpc|_imaging.cpython-38-x86_64-linux-gnu.so',\n",
       "   'ImagingResampleVertical_8bpc|_imaging.cpython-38-x86_64-linux-gnu.so',\n",
       "   '_int_free|libc.so.6',\n",
       "   'bilinear_filter|_imaging.cpython-38-x86_64-linux-gnu.so'],\n",
       "  'ToTensor': ['_int_free|libc.so.6',\n",
       "   'at::internal::set_thread_num|libtorch_cpu.so',\n",
       "   'PyBytes_FromStringAndSize|python3.8',\n",
       "   '_PyObject_GetMethod|python3.8',\n",
       "   'gomp_simple_barrier_wait|libgomp.so.1',\n",
       "   'PyObject_IsTrue|python3.8',\n",
       "   'method_vectorcall|python3.8',\n",
       "   'method_vectorcall_O|python3.8',\n",
       "   'tobytes|Image.py',\n",
       "   'ImagingPackRGB|_imaging.cpython-38-x86_64-linux-gnu.so',\n",
       "   'c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_kernel<at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#11}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#1}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(unsigned char)#1}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#11}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#1}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(unsigned char)#1}&&, long)::{lambda(char**long const*, long)#1}>(, signed char, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#11}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#1}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(unsigned char)#1} const&)::{lambda(char**long const*, long, long)#1}>|libtorch_cpu.so',\n",
       "   '__memmove_avx_unaligned_erms|libc.so.6',\n",
       "   '_Py_BuildValue_SizeT|python3.8',\n",
       "   '__posix_memalign|libc.so.6',\n",
       "   'c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(floatfloat)#1}, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(floatfloat)#1}&&, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2}>(, signed char, at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(floatfloat)#1}, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(floatfloat)#1}&&, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2} const&)::{lambda(char**long const*, long, long)#1}>|libtorch_cpu.so',\n",
       "   'c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(unsigned char)#1}, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<unsigned char>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(unsigned char)#1}&&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<unsigned char>)#2}&&, long)::{lambda(char**long const*, long)#2}>(, signed char, at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(unsigned char)#1}, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<unsigned char>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(unsigned char)#1}&&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<unsigned char>)#2}&&, long)::{lambda(char**long const*, long)#2} const&)::{lambda(char**long const*, long, long)#1}>|libtorch_cpu.so',\n",
       "   'munmap|libc.so.6',\n",
       "   'gomp_team_end|libgomp.so.1',\n",
       "   'method_vectorcall_VARARGS|python3.8',\n",
       "   'drop_gil|python3.8',\n",
       "   'gomp_team_barrier_wait_end|libgomp.so.1',\n",
       "   'take_gil|python3.8',\n",
       "   '_PyBytes_Resize|python3.8'],\n",
       "  'Normalize': ['c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(floatfloat)#1}, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(floatfloat)#1}&&, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2}>(, signed char, at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(floatfloat)#1}, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(floatfloat)#1}&&, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2} const&)::{lambda(char**long const*, long, long)#1}>|libtorch_cpu.so',\n",
       "   'munmap|libc.so.6',\n",
       "   'gomp_team_end|libgomp.so.1',\n",
       "   'normalize|functional.py',\n",
       "   'c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(float)#1}, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(float)#1}&&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2}>(, signed char, at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(float)#1}, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(float)#1}&&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2} const&)::{lambda(char**long const*, long, long)#1}>|libtorch_cpu.so',\n",
       "   'gomp_simple_barrier_wait|libgomp.so.1',\n",
       "   'gomp_team_barrier_wait_end|libgomp.so.1',\n",
       "   'c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_index_kernel<float, at::native::(anonymous namespace)::index_kernel(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(char*char*, long)#1}>(void, at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::(anonymous namespace)::index_kernel(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(char*char*, long)#1} const&, bool)::{lambda(char**long const*, long)#1}>(, signed char, at::native::(anonymous namespace)::cpu_index_kernel<float, at::native::(anonymous namespace)::index_kernel(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(char*char*, long)#1}>(void, at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::(anonymous namespace)::index_kernel(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(char*char*, long)#1} const&, bool)::{lambda(char**long const*, long)#1} const&)::{lambda(char**long const*, long, long)#1}>|libtorch_cpu.so',\n",
       "   'c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::mul_kernel(at::TensorIteratorBase&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(floatfloat)#1}, at::native::(anonymous namespace)::mul_kernel(at::TensorIteratorBase&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::mul_kernel(at::TensorIteratorBase&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(floatfloat)#1}&&, at::native::(anonymous namespace)::mul_kernel(at::TensorIteratorBase&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2}>(, signed char, at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::mul_kernel(at::TensorIteratorBase&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(floatfloat)#1}, at::native::(anonymous namespace)::mul_kernel(at::TensorIteratorBase&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::mul_kernel(at::TensorIteratorBase&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(floatfloat)#1}&&, at::native::(anonymous namespace)::mul_kernel(at::TensorIteratorBase&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2} const&)::{lambda(char**long const*, long, long)#1}>|libtorch_cpu.so',\n",
       "   'c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::add_kernel(at::TensorIteratorBase&, c10::Scalar const&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(floatfloat)#1}, at::native::(anonymous namespace)::add_kernel(at::TensorIteratorBase&, c10::Scalar const&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::add_kernel(at::TensorIteratorBase&, c10::Scalar const&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(floatfloat)#1}&&, at::native::(anonymous namespace)::add_kernel(at::TensorIteratorBase&, c10::Scalar const&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2}>(, signed char, at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::add_kernel(at::TensorIteratorBase&, c10::Scalar const&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(floatfloat)#1}, at::native::(anonymous namespace)::add_kernel(at::TensorIteratorBase&, c10::Scalar const&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::add_kernel(at::TensorIteratorBase&, c10::Scalar const&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(floatfloat)#1}&&, at::native::(anonymous namespace)::add_kernel(at::TensorIteratorBase&, c10::Scalar const&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2} const&)::{lambda(char**long const*, long, long)#1}>|libtorch_cpu.so']},\n",
       " 'common_funcs_to_op': {'munmap|libc.so.6': ['BatchCollator',\n",
       "   'ToTensor',\n",
       "   'Normalize'],\n",
       "  'gomp_team_end|libgomp.so.1': ['BatchCollator', 'ToTensor', 'Normalize'],\n",
       "  'c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(float)#1}, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(float)#1}&&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2}>(, signed char, at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(float)#1}, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(float)#1}&&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2} const&)::{lambda(char**long const*, long, long)#1}>|libtorch_cpu.so': ['BatchCollator',\n",
       "   'Normalize'],\n",
       "  'gomp_simple_barrier_wait|libgomp.so.1': ['BatchCollator',\n",
       "   'ToTensor',\n",
       "   'Normalize'],\n",
       "  'gomp_team_barrier_wait_end|libgomp.so.1': ['BatchCollator',\n",
       "   'ToTensor',\n",
       "   'Normalize'],\n",
       "  '_int_free|libc.so.6': ['RandomHorizontalFlip', 'Resize', 'ToTensor'],\n",
       "  'method_vectorcall|python3.8': ['Load', 'ToTensor'],\n",
       "  'PyObject_IsTrue|python3.8': ['Load', 'ToTensor'],\n",
       "  '__memmove_avx_unaligned_erms|libc.so.6': ['Load', 'ToTensor'],\n",
       "  'c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(floatfloat)#1}, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(floatfloat)#1}&&, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2}>(, signed char, at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(floatfloat)#1}, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(floatfloat)#1}&&, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2} const&)::{lambda(char**long const*, long, long)#1}>|libtorch_cpu.so': ['ToTensor',\n",
       "   'Normalize']}}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Function', 'CPU Time (s)', 'Retiring', 'Front-End Bound',\n",
       "       'Bad Speculation', 'L1 Bound', 'L2 Bound', 'L3 Bound',\n",
       "       'Memory Bandwidth', 'Local Memory', 'Remote Memory', 'Remote Cache',\n",
       "       'Store Bound', 'Core Bound', 'CPU Time %', 'config'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print columns of hw_df\n",
    "hw_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:  4\n",
      "\tPython func:  Collation\n",
      "\t\tC/C++ func:  munmap\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  gomp_team_end\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(float)#1}, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(float)#1}&&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2}>(, signed char, at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(float)#1}, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(float)#1}&&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2} const&)::{lambda(char**long const*, long, long)#1}>\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  [OpenMP worker]\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  gomp_simple_barrier_wait\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#1}, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#1}&&, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#2}&&, long)::{lambda(char**long const*, long)#2}>(, signed char, at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#1}, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#1}&&, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#2}&&, long)::{lambda(char**long const*, long)#2} const&)::{lambda(char**long const*, long, long)#1}>\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  _ZN2at8internal15invoke_parallelIZNS_18TensorIteratorBase8for_eachEN3c1012function_refIFvPPcPKlllEEElEUlllE_EEvlllRKT_._omp_fn.0\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  gomp_finish_task\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  gomp_team_barrier_wait_end\n",
      "\t\t\tEmpty\n",
      "\tPython func:  Load\n",
      "\t\tC/C++ func:  __GI___libc_malloc\n",
      "\t\tC/C++ func:  method_vectorcall\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  PyObject_IsTrue\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  ImagingJpegDecode\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  jpeg_read_scanlines\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  jpeg_fill_bit_buffer\n",
      "\t\tC/C++ func:  __libc_calloc\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  jpeg_idct_16x16\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  ycc_rgb_convert\n",
      "\t\tC/C++ func:  PyObject_IsInstance\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  ImagingUnpackRGB\n",
      "\t\tC/C++ func:  __memset_avx2_unaligned_erms\n",
      "\t\tC/C++ func:  bytes_concat\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  process_data_simple_main\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  load\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  method_vectorcall_FASTCALL\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  decompress_onepass\n",
      "\t\tC/C++ func:  decode_mcu\n",
      "\t\tC/C++ func:  __memmove_avx_unaligned_erms\n",
      "\t\t\tScale factor:  0.5869609990789648\n",
      "\t\tC/C++ func:  _copy\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  jpeg_idct_islow\n",
      "\t\tC/C++ func:  _decode\n",
      "\t\t\tEmpty\n",
      "\tPython func:  Normalize\n",
      "\t\tC/C++ func:  c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(floatfloat)#1}, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(floatfloat)#1}&&, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2}>(, signed char, at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(floatfloat)#1}, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(floatfloat)#1}&&, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2} const&)::{lambda(char**long const*, long, long)#1}>\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  munmap\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  gomp_team_end\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  normalize\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(float)#1}, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(float)#1}&&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2}>(, signed char, at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(float)#1}, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(float)#1}&&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2} const&)::{lambda(char**long const*, long, long)#1}>\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  gomp_simple_barrier_wait\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  gomp_team_barrier_wait_end\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_index_kernel<float, at::native::(anonymous namespace)::index_kernel(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(char*char*, long)#1}>(void, at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::(anonymous namespace)::index_kernel(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(char*char*, long)#1} const&, bool)::{lambda(char**long const*, long)#1}>(, signed char, at::native::(anonymous namespace)::cpu_index_kernel<float, at::native::(anonymous namespace)::index_kernel(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(char*char*, long)#1}>(void, at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::(anonymous namespace)::index_kernel(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda()#1}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(char*char*, long)#1} const&, bool)::{lambda(char**long const*, long)#1} const&)::{lambda(char**long const*, long, long)#1}>\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::mul_kernel(at::TensorIteratorBase&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(floatfloat)#1}, at::native::(anonymous namespace)::mul_kernel(at::TensorIteratorBase&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::mul_kernel(at::TensorIteratorBase&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(floatfloat)#1}&&, at::native::(anonymous namespace)::mul_kernel(at::TensorIteratorBase&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2}>(, signed char, at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::mul_kernel(at::TensorIteratorBase&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(floatfloat)#1}, at::native::(anonymous namespace)::mul_kernel(at::TensorIteratorBase&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::mul_kernel(at::TensorIteratorBase&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(floatfloat)#1}&&, at::native::(anonymous namespace)::mul_kernel(at::TensorIteratorBase&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2} const&)::{lambda(char**long const*, long, long)#1}>\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::add_kernel(at::TensorIteratorBase&, c10::Scalar const&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(floatfloat)#1}, at::native::(anonymous namespace)::add_kernel(at::TensorIteratorBase&, c10::Scalar const&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::add_kernel(at::TensorIteratorBase&, c10::Scalar const&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(floatfloat)#1}&&, at::native::(anonymous namespace)::add_kernel(at::TensorIteratorBase&, c10::Scalar const&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2}>(, signed char, at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::add_kernel(at::TensorIteratorBase&, c10::Scalar const&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(floatfloat)#1}, at::native::(anonymous namespace)::add_kernel(at::TensorIteratorBase&, c10::Scalar const&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::add_kernel(at::TensorIteratorBase&, c10::Scalar const&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(floatfloat)#1}&&, at::native::(anonymous namespace)::add_kernel(at::TensorIteratorBase&, c10::Scalar const&)::{lambda()#2}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2} const&)::{lambda(char**long const*, long, long)#1}>\n",
      "\t\t\tEmpty\n",
      "\tPython func:  RandomHorizontalFlip\n",
      "\t\tC/C++ func:  ImagingFlipLeftRight\n",
      "\t\tC/C++ func:  _int_free\n",
      "\t\t\tScale factor:  0.03113324725852961\n",
      "\tPython func:  Resize\n",
      "\t\tC/C++ func:  ImagingResampleHorizontal_8bpc\n",
      "\t\tC/C++ func:  ImagingResampleVertical_8bpc\n",
      "\t\tC/C++ func:  _int_free\n",
      "\t\t\tScale factor:  0.5647918042844032\n",
      "\t\tC/C++ func:  bilinear_filter\n",
      "\t\t\tEmpty\n",
      "\tPython func:  ToTensor\n",
      "\t\tC/C++ func:  _int_free\n",
      "\t\t\tScale factor:  0.40407494845706715\n",
      "\t\tC/C++ func:  at::internal::set_thread_num\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  PyBytes_FromStringAndSize\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  _PyObject_GetMethod\n",
      "\t\tC/C++ func:  gomp_simple_barrier_wait\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  PyObject_IsTrue\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  method_vectorcall\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  method_vectorcall_O\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  tobytes\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  ImagingPackRGB\n",
      "\t\tC/C++ func:  c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_kernel<at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#11}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#1}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(unsigned char)#1}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#11}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#1}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(unsigned char)#1}&&, long)::{lambda(char**long const*, long)#1}>(, signed char, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#11}::operator()(void) const::{lambda()#8}::operator()(void) const::{lambda()#1}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(unsigned char)#1} const&)::{lambda(char**long const*, long, long)#1}>\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  __memmove_avx_unaligned_erms\n",
      "\t\t\tScale factor:  0.4130390009210352\n",
      "\t\tC/C++ func:  _Py_BuildValue_SizeT\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  __posix_memalign\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(floatfloat)#1}, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(floatfloat)#1}&&, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2}>(, signed char, at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(floatfloat)#1}, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(floatfloat)#1}&&, at::native::(anonymous namespace)::div_true_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()(void) const::{lambda()#4}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<float>at::vec::(anonymous namespace)::Vectorized<float>)#2}&&, long)::{lambda(char**long const*, long)#2} const&)::{lambda(char**long const*, long, long)#1}>\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  c10::function_ref<void (char**, long const*, long, long)>::callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(unsigned char)#1}, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<unsigned char>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(unsigned char)#1}&&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<unsigned char>)#2}&&, long)::{lambda(char**long const*, long)#2}>(, signed char, at::native::(anonymous namespace)::cpu_kernel_vec<(bool)1, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(unsigned char)#1}, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<unsigned char>)#2}>(void, at::TensorIteratorBase&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(unsigned char)#1}&&, at::native::(anonymous namespace)::copy_kernel(at::TensorIterator&, bool)::{lambda()#6}::operator()(void) const::{lambda()#2}::operator()(void) const::{lambda(at::vec::(anonymous namespace)::Vectorized<unsigned char>)#2}&&, long)::{lambda(char**long const*, long)#2} const&)::{lambda(char**long const*, long, long)#1}>\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  munmap\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  gomp_team_end\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  method_vectorcall_VARARGS\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  drop_gil\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  gomp_team_barrier_wait_end\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  take_gil\n",
      "\t\t\tEmpty\n",
      "\t\tC/C++ func:  _PyBytes_Resize\n",
      "\t\t\tEmpty\n"
     ]
    }
   ],
   "source": [
    "hw_columns_to_su = ['CPU Time (s)', 'Retiring', 'Front-End Bound',\n",
    "       'Bad Speculation', 'L1 Bound', 'L2 Bound', 'L3 Bound',\n",
    "       'Memory Bandwidth', 'Local Memory', 'Remote Memory', 'Remote Cache',\n",
    "       'Store Bound', 'Core Bound', 'CPU Time %']\n",
    "combined_df = pd.DataFrame()\n",
    "# loop config column in high_level_df\n",
    "for config in high_level_df.index:\n",
    "    print('Config: ',config)\n",
    "    config_df = pd.DataFrame()\n",
    "    # loop through each column in high_level_df\n",
    "    for python_func in high_level_df.columns:\n",
    "        print('\\tPython func: ',python_func)\n",
    "        # get the mapping function for the python_func\n",
    "        if python_func == \"Collation\":\n",
    "            c_cppfuncs = mapping_funcs['op_to_func']['BatchCollator']\n",
    "        else:\n",
    "            c_cppfuncs = mapping_funcs['op_to_func'][python_func]\n",
    "        python_df = pd.DataFrame()\n",
    "        # loop through each column in mapping_func\n",
    "        for c in c_cppfuncs:\n",
    "            # rename the c func if it exists in the func_rename dict\n",
    "            func_only = c.split('|')[0]\n",
    "            print('\\t\\tC/C++ func: ',func_only)\n",
    "            # get the column in hw_df that matches the c func and config\n",
    "            hw_df_col = hw_df[(hw_df['config'] == config) & (hw_df['Function'] == func_only)].copy()\n",
    "            # check if empty\n",
    "            if hw_df_col.empty:\n",
    "                print('\\t\\t\\tEmpty')\n",
    "                continue\n",
    "            # print('\\t\\t\\tHW data: ',hw_df_col)\n",
    "            # drop the config and function column\n",
    "            hw_df_col = hw_df_col.drop(columns=['config', 'Function'])\n",
    "            # multiply the hw_df_col by the percentage of the python_func if its a common op\n",
    "            if c in mapping_funcs['common_funcs_to_op']:\n",
    "                ops = mapping_funcs['common_funcs_to_op'][c]\n",
    "                base_total = 0\n",
    "                for op in ops:\n",
    "                    base_total += high_level_df.loc[config, op]\n",
    "                scale_factor = high_level_df.loc[config, python_func] / base_total\n",
    "                print('\\t\\t\\tScale factor: ',scale_factor)\n",
    "                hw_df_col = hw_df_col.mul(scale_factor)\n",
    "            # concat the hw_df_col to the python_df\n",
    "            python_df = pd.concat([python_df, hw_df_col])\n",
    "        # sum the python_df and print\n",
    "        python_series = python_df.sum()\n",
    "        python_df = pd.DataFrame({python_func: python_series})\n",
    "        # concat the python_df to the config_df\n",
    "        config_df = pd.concat([config_df, python_df],axis=1)\n",
    "\n",
    "    # transpose the config_df\n",
    "    config_df = config_df.T\n",
    "    # make index a column with name 'Function'\n",
    "    config_df = config_df.reset_index().rename(columns={'index': 'Function'})\n",
    "    # add config column to config_df\n",
    "    config_df['config'] = config\n",
    "    # concat the config_df to the combined_df\n",
    "    combined_df = pd.concat([combined_df, config_df])\n",
    "# reset the index\n",
    "combined_df = combined_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Function</th>\n",
       "      <th>CPU Time (s)</th>\n",
       "      <th>Retiring</th>\n",
       "      <th>Front-End Bound</th>\n",
       "      <th>Bad Speculation</th>\n",
       "      <th>L1 Bound</th>\n",
       "      <th>L2 Bound</th>\n",
       "      <th>L3 Bound</th>\n",
       "      <th>Memory Bandwidth</th>\n",
       "      <th>Local Memory</th>\n",
       "      <th>Remote Memory</th>\n",
       "      <th>Remote Cache</th>\n",
       "      <th>Store Bound</th>\n",
       "      <th>Core Bound</th>\n",
       "      <th>CPU Time %</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Collation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Load</td>\n",
       "      <td>1026.769265</td>\n",
       "      <td>1.234057e+14</td>\n",
       "      <td>2.749928e+13</td>\n",
       "      <td>5.283827e+13</td>\n",
       "      <td>2.726350e+13</td>\n",
       "      <td>1.897129e+12</td>\n",
       "      <td>6.425893e+12</td>\n",
       "      <td>2.752266e+13</td>\n",
       "      <td>6.931421e+11</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.393110e+13</td>\n",
       "      <td>2.649822e+13</td>\n",
       "      <td>5.666894</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Normalize</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomHorizontalFlip</td>\n",
       "      <td>35.519053</td>\n",
       "      <td>9.908453e+12</td>\n",
       "      <td>4.390892e+11</td>\n",
       "      <td>2.418705e+10</td>\n",
       "      <td>4.760800e+11</td>\n",
       "      <td>2.341120e+10</td>\n",
       "      <td>2.557468e+11</td>\n",
       "      <td>6.269831e+12</td>\n",
       "      <td>2.233798e+12</td>\n",
       "      <td>1.934964e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.933311e+11</td>\n",
       "      <td>6.584474e+11</td>\n",
       "      <td>0.196035</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Resize</td>\n",
       "      <td>1037.367406</td>\n",
       "      <td>2.856161e+14</td>\n",
       "      <td>6.767853e+12</td>\n",
       "      <td>4.387800e+11</td>\n",
       "      <td>4.229519e+12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.134005e+12</td>\n",
       "      <td>5.315517e+12</td>\n",
       "      <td>7.396577e+12</td>\n",
       "      <td>3.510240e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.293942e+11</td>\n",
       "      <td>5.622829e+13</td>\n",
       "      <td>5.725387</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Function  CPU Time (s)      Retiring  Front-End Bound  \\\n",
       "0             Collation           NaN           NaN              NaN   \n",
       "1                  Load   1026.769265  1.234057e+14     2.749928e+13   \n",
       "2             Normalize           NaN           NaN              NaN   \n",
       "3  RandomHorizontalFlip     35.519053  9.908453e+12     4.390892e+11   \n",
       "4                Resize   1037.367406  2.856161e+14     6.767853e+12   \n",
       "\n",
       "   Bad Speculation      L1 Bound      L2 Bound      L3 Bound  \\\n",
       "0              NaN           NaN           NaN           NaN   \n",
       "1     5.283827e+13  2.726350e+13  1.897129e+12  6.425893e+12   \n",
       "2              NaN           NaN           NaN           NaN   \n",
       "3     2.418705e+10  4.760800e+11  2.341120e+10  2.557468e+11   \n",
       "4     4.387800e+11  4.229519e+12  0.000000e+00  2.134005e+12   \n",
       "\n",
       "   Memory Bandwidth  Local Memory  Remote Memory  Remote Cache   Store Bound  \\\n",
       "0               NaN           NaN            NaN           NaN           NaN   \n",
       "1      2.752266e+13  6.931421e+11   0.000000e+00           0.0  1.393110e+13   \n",
       "2               NaN           NaN            NaN           NaN           NaN   \n",
       "3      6.269831e+12  2.233798e+12   1.934964e+10           0.0  2.933311e+11   \n",
       "4      5.315517e+12  7.396577e+12   3.510240e+11           0.0  2.293942e+11   \n",
       "\n",
       "     Core Bound  CPU Time %  config  \n",
       "0           NaN         NaN       4  \n",
       "1  2.649822e+13    5.666894       4  \n",
       "2           NaN         NaN       4  \n",
       "3  6.584474e+11    0.196035       4  \n",
       "4  5.622829e+13    5.725387       4  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Function', 'CPU Time (s)', 'Retiring', 'Front-End Bound',\n",
       "       'Bad Speculation', 'L1 Bound', 'L2 Bound', 'L3 Bound',\n",
       "       'Memory Bandwidth', 'Local Memory', 'Remote Memory', 'Remote Cache',\n",
       "       'Store Bound', 'Core Bound', 'CPU Time %', 'config'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print columns of combined_df\n",
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Function</th>\n",
       "      <th>CPU Time (s)</th>\n",
       "      <th>Retiring</th>\n",
       "      <th>Front-End Bound</th>\n",
       "      <th>Bad Speculation</th>\n",
       "      <th>L1 Bound</th>\n",
       "      <th>L2 Bound</th>\n",
       "      <th>L3 Bound</th>\n",
       "      <th>Memory Bandwidth</th>\n",
       "      <th>Local Memory</th>\n",
       "      <th>Remote Memory</th>\n",
       "      <th>Remote Cache</th>\n",
       "      <th>Store Bound</th>\n",
       "      <th>Core Bound</th>\n",
       "      <th>CPU Time %</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Collation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Load</td>\n",
       "      <td>1026.769265</td>\n",
       "      <td>1.234057e+14</td>\n",
       "      <td>2.749928e+13</td>\n",
       "      <td>5.283827e+13</td>\n",
       "      <td>2.726350e+13</td>\n",
       "      <td>1.897129e+12</td>\n",
       "      <td>6.425893e+12</td>\n",
       "      <td>2.752266e+13</td>\n",
       "      <td>6.931421e+11</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.393110e+13</td>\n",
       "      <td>2.649822e+13</td>\n",
       "      <td>5.666894</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Normalize</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomHorizontalFlip</td>\n",
       "      <td>35.519053</td>\n",
       "      <td>9.908453e+12</td>\n",
       "      <td>4.390892e+11</td>\n",
       "      <td>2.418705e+10</td>\n",
       "      <td>4.760800e+11</td>\n",
       "      <td>2.341120e+10</td>\n",
       "      <td>2.557468e+11</td>\n",
       "      <td>6.269831e+12</td>\n",
       "      <td>2.233798e+12</td>\n",
       "      <td>1.934964e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.933311e+11</td>\n",
       "      <td>6.584474e+11</td>\n",
       "      <td>0.196035</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Resize</td>\n",
       "      <td>1037.367406</td>\n",
       "      <td>2.856161e+14</td>\n",
       "      <td>6.767853e+12</td>\n",
       "      <td>4.387800e+11</td>\n",
       "      <td>4.229519e+12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.134005e+12</td>\n",
       "      <td>5.315517e+12</td>\n",
       "      <td>7.396577e+12</td>\n",
       "      <td>3.510240e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.293942e+11</td>\n",
       "      <td>5.622829e+13</td>\n",
       "      <td>5.725387</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ToTensor</td>\n",
       "      <td>140.365276</td>\n",
       "      <td>2.068976e+13</td>\n",
       "      <td>8.208828e+12</td>\n",
       "      <td>2.279578e+12</td>\n",
       "      <td>5.431247e+12</td>\n",
       "      <td>1.291311e+12</td>\n",
       "      <td>4.916942e+12</td>\n",
       "      <td>1.656600e+13</td>\n",
       "      <td>6.568854e+12</td>\n",
       "      <td>2.511368e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.494394e+12</td>\n",
       "      <td>1.878772e+12</td>\n",
       "      <td>0.774697</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Function  CPU Time (s)      Retiring  Front-End Bound  \\\n",
       "0             Collation           NaN           NaN              NaN   \n",
       "1                  Load   1026.769265  1.234057e+14     2.749928e+13   \n",
       "2             Normalize           NaN           NaN              NaN   \n",
       "3  RandomHorizontalFlip     35.519053  9.908453e+12     4.390892e+11   \n",
       "4                Resize   1037.367406  2.856161e+14     6.767853e+12   \n",
       "5              ToTensor    140.365276  2.068976e+13     8.208828e+12   \n",
       "\n",
       "   Bad Speculation      L1 Bound      L2 Bound      L3 Bound  \\\n",
       "0              NaN           NaN           NaN           NaN   \n",
       "1     5.283827e+13  2.726350e+13  1.897129e+12  6.425893e+12   \n",
       "2              NaN           NaN           NaN           NaN   \n",
       "3     2.418705e+10  4.760800e+11  2.341120e+10  2.557468e+11   \n",
       "4     4.387800e+11  4.229519e+12  0.000000e+00  2.134005e+12   \n",
       "5     2.279578e+12  5.431247e+12  1.291311e+12  4.916942e+12   \n",
       "\n",
       "   Memory Bandwidth  Local Memory  Remote Memory  Remote Cache   Store Bound  \\\n",
       "0               NaN           NaN            NaN           NaN           NaN   \n",
       "1      2.752266e+13  6.931421e+11   0.000000e+00           0.0  1.393110e+13   \n",
       "2               NaN           NaN            NaN           NaN           NaN   \n",
       "3      6.269831e+12  2.233798e+12   1.934964e+10           0.0  2.933311e+11   \n",
       "4      5.315517e+12  7.396577e+12   3.510240e+11           0.0  2.293942e+11   \n",
       "5      1.656600e+13  6.568854e+12   2.511368e+11           0.0  8.494394e+12   \n",
       "\n",
       "     Core Bound  CPU Time %  config  \n",
       "0           NaN         NaN       4  \n",
       "1  2.649822e+13    5.666894       4  \n",
       "2           NaN         NaN       4  \n",
       "3  6.584474e+11    0.196035       4  \n",
       "4  5.622829e+13    5.725387       4  \n",
       "5  1.878772e+12    0.774697       4  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head(len(combined_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Time (s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retiring\n",
      "Front-End Bound\n",
      "Bad Speculation\n",
      "L1 Bound\n",
      "L2 Bound\n",
      "L3 Bound\n",
      "Memory Bandwidth\n",
      "Local Memory\n",
      "Remote Memory\n",
      "Remote Cache\n",
      "Store Bound\n",
      "Core Bound\n",
      "CPU Time %\n"
     ]
    }
   ],
   "source": [
    "for col in combined_df.columns:\n",
    "    if col == 'config' or col == 'Function':\n",
    "        continue\n",
    "    print(col)\n",
    "    # pivot table\n",
    "    df_pivot = combined_df.pivot(index='config', columns='Function', values=col)\n",
    "    # sort the index using natsort\n",
    "    df_pivot = df_pivot.reindex(natsort.natsorted(df_pivot.index))\n",
    "    # find lowest value in entire dataframe and normalize the dataframe by dividing each value by the lowest value suhc that its not 0\n",
    "    ax = df_pivot.plot(kind='bar', stacked=True, figsize=(12,6),colormap='jet')\n",
    "    # for c in ax.containers:\n",
    "    #     ax.bar_label(c, label_type='center')\n",
    "    # make x axis label horizontal\n",
    "    plt.xticks(rotation=0)\n",
    "    # add x axis label\n",
    "    plt.xlabel('Dataloaders')\n",
    "    # add y axis label\n",
    "    plt.ylabel(f\"{col}\")\n",
    "    # add title to legend\n",
    "    plt.title(f\"Hardware metric for preprocessing operations\")    \n",
    "    # reverse legend order\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles[::-1], labels[::-1],loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "    plt.subplots_adjust(right=0.70)\n",
    "    plt.savefig(f'./mapped_funcs_metric_figs/{col}.png')\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
